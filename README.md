# ğŸ” `rag_local_pdfs` 
### â€” Retrieval-Augmented Generation with Local PDFs

This project implements a **Retrieval-Augmented Generation (RAG)** system using **local PDF documents** as the knowledge base. Built with **LangChain**, **ChromaDB**, and a **local LLM via Ollama**, this setup enables question-answering over your own documents using efficient vector search and contextual responses from LLMs.

---
![Sample illustration, showing how data flow from source to Vector db to Response to the User](https://github.com/shekar369/rag_local_pdfs/blob/main/SImple_RAG.png)

## ğŸš€ Features

- ğŸ“„ **PDF Loader**: Extracts text from PDF files in your local `data/` directory.
- ğŸ§© **Text Chunking**: Splits text into overlapping chunks using `RecursiveCharacterTextSplitter` to preserve context.
- ğŸ“† **Vector Store (ChromaDB)**: Stores and indexes embeddings for fast similarity search.
- ğŸ§  **Local LLM via Ollama**: Generates contextual answers using lightweight models like Mistral, all on your local machine.
- âœ… **Testing Suite**: Validate system accuracy using test queries.

---

## ğŸ—‚ï¸ Project Structure

```
rag_local_pdfs/
â”‚
â”œâ”€â”€ data/                      # Local PDFs for ingestion
â”œâ”€â”€ get_embedding_function.py  # Defines embedding logic
â”œâ”€â”€ populate_database.py       # Loads, chunks, embeds, and stores PDFs
â”œâ”€â”€ query_data.py              # Queries ChromaDB and returns LLM response
â”œâ”€â”€ test_rag.py                # Test queries to validate responses
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md
```

## âš™ï¸ Setup Instructions
### 1. ğŸ“¥ Clone the repository
```git clone https://github.com/your-username/rag_local_pdfs.git
cd rag_local_pdfs
```

### 2. ğŸ§ª Create virtual environment & install dependencies
```
python -m venv venv
source venv/bin/activate   # on Windows use venv\Scripts\activate
pip install -r requirements.txt
```
### 3. ğŸ“‚ Add PDFs to data/ folder
Put your PDF files in the data/ directory for ingestion.

### 4. ğŸ§  Install and run Ollama
**Install Ollama from https://ollama.com, then run:**
```
ollama run llama 3.2
```
Make sure Ollama is running in the background.

### 5. ğŸ§¬ Populate the vector database
```
python populate_database.py
```
### 6. ğŸ” Query your documents
```python query_data.py "What is the summary of the document?"```

### 7. âœ… Run tests
```python test_rag.py```<br/><br/>

**ğŸ§  Local LLM Support**
  
Uses Ollama for running models like:<br/>
-mistral<br/>
-llama2<br/>
-gemma<br/>

Change model name in query_data.py if you prefer a different LLM.
  
## ğŸ“Œ Future Enhancements<br/>
-GUI or Streamlit interface<br/>
-Multi-file PDF summarization<br/>
-Real-time chat over documents<br/>
-RAG + Agent framework (e.g., LangGraph or CrewAI)<br/>
