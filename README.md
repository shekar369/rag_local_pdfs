# Understanding LLMs & RAG for Enterprise AI

### Presented by: **Shekar Kaki**  
**AI Engineer | Data Engineer | Python Developer | Application Architect**  
üåê [Portfolio](https://skaki.lovable.app) | üíº [LinkedIn](https://www.linkedin.com/in/shekar-kaki-8a2b85a0)    

---
![Understand what is LLM first](https://github.com/shekar369/rag_local_pdfs/blob/main/media/what_is_an_LLM.png)
## 1. What is an LLM?

A **Large Language Model (LLM)** is an advanced AI system trained to understand and generate human-like text.  
It learns from massive public datasets such as Wikipedia, books, and websites.  

**Key capabilities:**
- Answering questions  
- Summarizing content  
- Translating languages  
- Engaging in human-like conversation  

**Examples:** ChatGPT, Google Gemini, Claude, LLaMA

---

## 2. The Limitations of LLMs in Enterprises
While LLMs are powerful, they come with limitations in business environments:

- No access to internal documents, company policies, or private databases  
- Cannot provide accurate, up-to-date answers about internal or proprietary information  
- Lack of context about your organization limits their usefulness in real-world applications  

---

## 3. Why RAG is Needed

**Retrieval-Augmented Generation (RAG)** bridges the gap between LLMs and enterprise knowledge:

- Retrieves relevant internal data in real-time  
- Augments the LLM's responses with accurate, business-specific context  
- Delivers precise and trustworthy answers based on your own content  

**Result:** Smarter, enterprise-ready AI that truly understands your business
![How Enterprice RAG Works](https://github.com/shekar369/rag_local_pdfs/blob/main/media/Enterprice_RAG.png)

#  `rag_local_pdfs` 
### ‚Äî Retrieval-Augmented Generation with Local PDFs

This project implements a **Retrieval-Augmented Generation (RAG)** system using **local PDF documents** as the knowledge base. Built with **LangChain**, **ChromaDB**, and a **local LLM via Ollama**, this setup enables question-answering over your own documents using efficient vector search and contextual responses from LLMs.

---
![Sample illustration, showing how data flow from source to Vector db to Response to the User](https://github.com/shekar369/rag_local_pdfs/blob/main/SImple_RAG.png)


## Features

-  **PDF Loader**: Extracts text from PDF files in your local `data/` directory.
-  **Text Chunking**: Splits text into overlapping chunks using `RecursiveCharacterTextSplitter` to preserve context.
-  **Vector Store (ChromaDB)**: Stores and indexes embeddings for fast similarity search.
-  **Local LLM via Ollama**: Generates contextual answers using lightweight models like Mistral, all on your local machine.
-  **Testing Suite**: Validate system accuracy using test queries.

---

##  Project Structure

```
rag_local_pdfs/
‚îÇ
‚îú‚îÄ‚îÄ chroma/                    # chroma directory will be added when we run populate_database.py
‚îú‚îÄ‚îÄ data/                      # Local PDFs for ingestion
‚îú‚îÄ‚îÄ media/                     # Concept images, documents
‚îú‚îÄ‚îÄ get_embedding_function.py  # Defines embedding logic
‚îú‚îÄ‚îÄ populate_database.py       # Loads, chunks, embeds, and stores PDFs
‚îú‚îÄ‚îÄ query_data.py              # Queries ChromaDB and returns LLM response
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

##  Setup Instructions
### 1. Clone the repository
```git clone https://github.com/your-username/rag_local_pdfs.git
cd rag_local_pdfs
```

### 2. Create virtual environment & install dependencies
```
python -m venv venv
source venv/bin/activate   # on Windows use venv\Scripts\activate
pip install -r requirements.txt
```
### 3. Add PDFs to data/ folder
Put your PDF files in the data/ directory for ingestion.

### 4. Install and run Ollama server. For running LLM Locally.
**Install Ollama from https://ollama.com, then run:**
Below step will download model
```
ollama run llama 3.2
```
Make sure Ollama is running in the background. By checking http://127.0.0.1:11434/ you should see "Ollama is running" in the page

### 5. Populate the vector database
```
python populate_database.py
```
----
![Sample illustration, showing how data flow from source to Vector db to Response to the User](https://github.com/shekar369/rag_local_pdfs/blob/main/Rag_original-chunks-embedding.png)
### 6. Query your documents
```python query_data.py "What is the summary of the document?"```

![Sample illustration, showing how response comes from Vector db to Response to the User](https://github.com/shekar369/rag_local_pdfs/blob/main/rag_local_pdfs/Rag_query-vector-response.png)

### Successful setup will give you response from the pdf you added by giving summary

** Local LLM Support**
  
Uses Ollama for running models like:<br/>
-llama3.2<br/>
-gemma<br/>

Change model name in query_data.py if you prefer a different LLM.
  
## üìå Future Enhancements<br/>
-GUI or Streamlit interface<br/>
-Multi-file PDF summarization<br/>
-Real-time chat over documents<br/>
-RAG + Agent framework (e.g., LangGraph or CrewAI)<br/>
